"""
Grounding DINO Detection Model Manager
Grounding DINO í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ê°ì²´ íƒì§€ ëª¨ë¸ ê´€ë¦¬ì
Hugging Face Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
"""
import torch
import logging
import numpy as np
from typing import Dict, Any, Optional, List
from PIL import Image
from fastapi import HTTPException

from ..base_model import BaseModel, ModelType, TaskType

logger = logging.getLogger(__name__)


class GroundingDINOManager(BaseModel):
    """
    Grounding DINO ê°ì²´ íƒì§€ ëª¨ë¸ ê´€ë¦¬ì
    í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ zero-shot ê°ì²´ íƒì§€
    Hugging Face Hubì—ì„œ ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ
    """

    def __init__(self):
        """Grounding DINO ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        super().__init__()
        self.model_type = ModelType.DETECTION
        self.task_type = TaskType.BBOX
        self.processor = None
        self.model_id = None

    def load_model(self, model_path: str = None, **kwargs):
        """
        Grounding DINO ëª¨ë¸ ë¡œë”© (Hugging Face Hub)

        Args:
            model_path (str, optional): Hugging Face model ID
                (ê¸°ë³¸ê°’: "IDEA-Research/grounding-dino-tiny")
            **kwargs:
                - model_id (str): ëª¨ë¸ ID (model_path ëŒ€ì‹  ì‚¬ìš© ê°€ëŠ¥)
                - enable_compile (bool): torch.compile() ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
                  â†’ Windowsì—ì„œëŠ” Triton ë¯¸ì§€ì›ìœ¼ë¡œ ê¸°ë³¸ ë¹„í™œì„±í™”
        """
        try:
            # Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
            try:
                from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
            except ImportError:
                raise ImportError(
                    "transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                    "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install transformers"
                )

            # ëª¨ë¸ ID ê²°ì • (ìš°ì„ ìˆœìœ„: kwargs['model_id'] > model_path > ê¸°ë³¸ê°’)
            self.model_id = kwargs.get('model_id') or model_path or "IDEA-Research/grounding-dino-tiny"

            logger.info(f"ğŸ”„ Grounding DINO ëª¨ë¸ ë¡œë”© ì‹œì‘")
            logger.info(f"  - Model ID: {self.model_id}")
            logger.info(f"  - Source: Hugging Face Hub")

            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"  - Device: {device}")

            # Processorì™€ ëª¨ë¸ ë¡œë“œ
            logger.info(f"ğŸ“¥ Processor ë‹¤ìš´ë¡œë“œ ì¤‘...")
            self.processor = AutoProcessor.from_pretrained(self.model_id)

            logger.info(f"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            self.model = AutoModelForZeroShotObjectDetection.from_pretrained(self.model_id)

            # GPUë¡œ ì´ë™
            self.model.to(device)

            # torch.compile()ë¡œ ëª¨ë¸ ìµœì í™” (PyTorch 2.0+)
            # Windows í™˜ê²½ì—ì„œëŠ” Triton ë¯¸ì§€ì›ìœ¼ë¡œ ê¸°ë³¸ ë¹„í™œì„±í™”
            enable_compile = kwargs.get('enable_compile', False)  # ê¸°ë³¸ê°’: False (ì•ˆì •ì„± ìš°ì„ )

            if device == "cuda" and enable_compile:
                try:
                    logger.info(f"ğŸ”§ torch.compile()ë¡œ ëª¨ë¸ ìµœì í™” ì¤‘...")

                    # GPU í™˜ê²½ì— ìµœì í™”ëœ ì„¤ì •
                    compile_options = {
                        "mode": "max-autotune",  # GPU ìµœëŒ€ ì„±ëŠ¥
                        "fullgraph": False,  # í˜¸í™˜ì„± í–¥ìƒ
                    }

                    self.model = torch.compile(self.model, **compile_options)
                    logger.info(f"âœ… torch.compile() ì ìš© ì™„ë£Œ (ì²« ì¶”ë¡  ì‹œ ì»´íŒŒì¼ë¨)")

                except Exception as compile_error:
                    logger.warning(f"âš ï¸ torch.compile() ì ìš© ì‹¤íŒ¨: {str(compile_error)}")
                    logger.info(f"   ê¸°ë³¸ ëª¨ë¸ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤")
            else:
                if device == "cpu":
                    logger.info(f"â„¹ï¸ CPU ëª¨ë“œì—ì„œëŠ” torch.compile() ê±´ë„ˆëœ€")
                elif not enable_compile:
                    logger.info(f"â„¹ï¸ torch.compile() ë¹„í™œì„±í™”ë¨ (ì•ˆì •ì„± ìš°ì„ )")
                    logger.info(f"   ğŸ’¡ í™œì„±í™”í•˜ë ¤ë©´: load_model(enable_compile=True)")

            if device == "cuda":
                logger.info(f"âœ… Grounding DINO ëª¨ë¸ì„ GPUë¡œ ë¡œë“œ")
            else:
                logger.info(f"âœ… Grounding DINO ëª¨ë¸ì„ CPUë¡œ ë¡œë“œ")

            self.is_loaded = True
            logger.info(f"âœ… Grounding DINO ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

            return {
                "success": True,
                "message": f"Grounding DINO model loaded successfully from Hugging Face",
                "model_id": self.model_id,
                "supports_text_prompt": True,
                "device": device
            }

        except Exception as e:
            logger.error(f"âŒ Grounding DINO ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")

    def predict(self, image, **kwargs) -> Dict[str, Any]:
        """
        Grounding DINO ê°ì²´ íƒì§€ ì¶”ë¡  (Transformers API)

        Args:
            image: PIL Image ë˜ëŠ” numpy array
            **kwargs:
                - text_prompt (str): íƒì§€í•  ê°ì²´ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (ì˜ˆ: "person. car. dog.")
                - box_threshold (float): ë°•ìŠ¤ ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)
                - text_threshold (float): í…ìŠ¤íŠ¸ ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.25)

        Returns:
            Dict[str, Any]: íƒì§€ ê²°ê³¼
        """
        if not self.validate_model():
            raise HTTPException(status_code=400, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        try:
            # í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸
            text_prompt = kwargs.get('text_prompt')
            if not text_prompt:
                raise ValueError("text_promptê°€ í•„ìš”í•©ë‹ˆë‹¤ (ì˜ˆ: 'person. car. dog.')")

            box_threshold = kwargs.get('box_threshold', 0.3)
            text_threshold = kwargs.get('text_threshold', 0.25)

            # í”„ë¡¬í”„íŠ¸ì—ì„œ í´ë˜ìŠ¤ ìˆœì„œ ì¶”ì¶œ (ì˜ˆ: "person. helmet." â†’ ["person", "helmet"])
            prompt_classes = [cls.strip() for cls in text_prompt.split('.') if cls.strip()]

            logger.info(f"ğŸ” Grounding DINO ì¶”ë¡  ì‹œì‘")
            logger.info(f"  - í”„ë¡¬í”„íŠ¸: {text_prompt}")
            logger.info(f"  - í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤ ìˆœì„œ: {prompt_classes}")
            logger.info(f"  - Box threshold: {box_threshold}")
            logger.info(f"  - Text threshold: {text_threshold}")

            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if not isinstance(image, Image.Image):
                if isinstance(image, np.ndarray):
                    image = Image.fromarray(image)
                else:
                    raise ValueError("ì´ë¯¸ì§€ëŠ” PIL Image ë˜ëŠ” numpy arrayì—¬ì•¼ í•©ë‹ˆë‹¤")

            # RGB ë³€í™˜
            if image.mode != 'RGB':
                image = image.convert('RGB')

            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ì €ì¥
            original_size = image.size  # (width, height)
            logger.info(f"  - ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {original_size[0]}x{original_size[1]}")

            # ì¶”ë¡  ì†ë„ í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• (ê¸´ ì¶•ì„ 800pxë¡œ ì œí•œ)
            max_size = 800
            width, height = image.size

            if max(width, height) > max_size:
                # ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ë¦¬ì‚¬ì´ì§•
                if width > height:
                    new_width = max_size
                    new_height = int(height * (max_size / width))
                else:
                    new_height = max_size
                    new_width = int(width * (max_size / height))

                image = image.resize((new_width, new_height), Image.LANCZOS)
                logger.info(f"  - ë¦¬ì‚¬ì´ì§•ëœ í¬ê¸°: {new_width}x{new_height} (ì†ë„ ìµœì í™”)")
            else:
                logger.info(f"  - ë¦¬ì‚¬ì´ì§• ë¶ˆí•„ìš” (ì´ë¯¸ {max_size}px ì´í•˜)")

            # Processorë¡œ ì…ë ¥ ì „ì²˜ë¦¬
            device = next(self.model.parameters()).device

            # Transformers ë²„ì „ í™•ì¸
            import transformers

            inputs = self.processor(
                images=image,
                text=text_prompt,
                return_tensors="pt"
            ).to(device)

            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(**inputs)

            # í›„ì²˜ë¦¬ (Transformers API)
            # ë²„ì „ì— ë”°ë¼ threshold íŒŒë¼ë¯¸í„° ì§€ì› ì—¬ë¶€ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ

            try:
                # ìµœì‹  ë²„ì „: box_threshold, text_threshold ì§€ì›
                results = self.processor.post_process_grounded_object_detection(
                    outputs,
                    inputs.input_ids,
                    box_threshold=box_threshold,
                    text_threshold=text_threshold,
                    target_sizes=[image.size[::-1]]
                )[0]

                # ê²°ê³¼ ë³€í™˜ (ë‚´ì¥ threshold í•„í„°ë§ ì‚¬ìš©)
                detections = self._postprocess_results(
                    boxes=results["boxes"],
                    scores=results["scores"],
                    labels=results["labels"],
                    image_size=image.size,
                    prompt_classes=prompt_classes,
                    original_size=original_size
                )

            except TypeError as e:
                # êµ¬ë²„ì „: threshold íŒŒë¼ë¯¸í„° ë¯¸ì§€ì› - ìˆ˜ë™ í•„í„°ë§ (ê²°ê³¼ëŠ” ë™ì¼)
                logger.info(f"â„¹ï¸ Transformers {transformers.__version__} - threshold íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›, ìˆ˜ë™ í•„í„°ë§ ì‚¬ìš© (ê²°ê³¼ ë™ì¼)")

                results = self.processor.post_process_grounded_object_detection(
                    outputs,
                    inputs.input_ids,
                    target_sizes=[image.size[::-1]]
                )[0]

                # threshold ìˆ˜ë™ ì ìš©
                filtered_boxes = []
                filtered_scores = []
                filtered_labels = []

                for box, score, label in zip(results["boxes"], results["scores"], results["labels"]):
                    if float(score) >= box_threshold:
                        filtered_boxes.append(box)
                        filtered_scores.append(score)
                        filtered_labels.append(label)

                # ê²°ê³¼ ë³€í™˜
                if len(filtered_boxes) > 0:
                    detections = self._postprocess_results(
                        boxes=torch.stack(filtered_boxes),
                        scores=torch.stack(filtered_scores),
                        labels=filtered_labels,
                        image_size=image.size,
                        prompt_classes=prompt_classes,
                        original_size=original_size
                    )
                else:
                    detections = []

            logger.info(f"âœ… Grounding DINO ì¶”ë¡  ì™„ë£Œ - íƒì§€ëœ ê°ì²´: {len(detections)}ê°œ")

            return {
                "boxes": detections,
                "num_detections": len(detections),
                "task_type": "bbox",
                "model_type": "grounding_dino",
                "text_prompt": text_prompt,
                "prompt_classes": prompt_classes  # í”„ë¡¬í”„íŠ¸ ìˆœì„œ ì •ë³´ ì¶”ê°€
            }

        except Exception as e:
            logger.error(f"âŒ Grounding DINO ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
            raise HTTPException(status_code=500, detail=f"ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")

    def _postprocess_results(
        self,
        boxes: torch.Tensor,
        scores: torch.Tensor,
        labels: List[str],
        image_size: tuple,
        prompt_classes: List[str] = None,
        original_size: tuple = None
    ) -> List[Dict]:
        """
        Grounding DINO ê²°ê³¼ í›„ì²˜ë¦¬ (Transformers API)

        Args:
            boxes: íƒì§€ëœ ë°•ìŠ¤ (í”½ì…€ ì¢Œí‘œ xyxy format)
            scores: ì‹ ë¢°ë„ ì ìˆ˜
            labels: íƒì§€ëœ í…ìŠ¤íŠ¸ ë ˆì´ë¸”
            image_size: ì¶”ë¡ ì— ì‚¬ìš©ëœ ì´ë¯¸ì§€ í¬ê¸° (width, height)
            prompt_classes: í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤ ìˆœì„œ (ì˜ˆ: ["person", "helmet"])
            original_size: ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° (width, height) - ë¦¬ì‚¬ì´ì§• ì‹œ í•„ìš”

        Returns:
            List[Dict]: ë°•ìŠ¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        detections = []
        img_width, img_height = image_size

        # ì¢Œí‘œ ìŠ¤ì¼€ì¼ íŒ©í„° ê³„ì‚° (ë¦¬ì‚¬ì´ì§•ëœ ê²½ìš°)
        if original_size and original_size != image_size:
            scale_x = original_size[0] / img_width
            scale_y = original_size[1] / img_height
            logger.info(f"  - ì¢Œí‘œ ìŠ¤ì¼€ì¼ì—…: {scale_x:.2f}x (ê°€ë¡œ), {scale_y:.2f}x (ì„¸ë¡œ)")
        else:
            scale_x = 1.0
            scale_y = 1.0

        # ì •ê·œí™” ê³„ì‚°ì€ ì›ë³¸ í¬ê¸° ê¸°ì¤€
        norm_width = original_size[0] if original_size else img_width
        norm_height = original_size[1] if original_size else img_height

        # í”„ë¡¬í”„íŠ¸ í´ë˜ìŠ¤ ìˆœì„œë¡œ class_id ë§¤í•‘ ìƒì„±
        class_id_mapping = {}
        if prompt_classes:
            for idx, cls_name in enumerate(prompt_classes):
                class_id_mapping[cls_name.lower()] = idx
            logger.info(f"ğŸ“‹ í´ë˜ìŠ¤ ID ë§¤í•‘ (í”„ë¡¬í”„íŠ¸ ìˆœì„œ): {class_id_mapping}")

        # Tensorë¥¼ numpyë¡œ ë³€í™˜
        if isinstance(boxes, torch.Tensor):
            boxes = boxes.cpu().numpy()
        if isinstance(scores, torch.Tensor):
            scores = scores.cpu().numpy()

        for box, confidence, label in zip(boxes, scores, labels):
            # í”½ì…€ ì¢Œí‘œ xyxy format
            x_min, y_min, x_max, y_max = box

            # ì›ë³¸ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ì—… (ë¦¬ì‚¬ì´ì§•ëœ ê²½ìš°)
            x_min = x_min * scale_x
            y_min = y_min * scale_y
            x_max = x_max * scale_x
            y_max = y_max * scale_y

            # xywh í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            width = x_max - x_min
            height = y_max - y_min

            # ì •ê·œí™”ëœ ì¢Œí‘œ ê³„ì‚° (ì›ë³¸ í¬ê¸° ê¸°ì¤€)
            x_center_norm = ((x_min + x_max) / 2) / norm_width
            y_center_norm = ((y_min + y_max) / 2) / norm_height
            width_norm = width / norm_width
            height_norm = height / norm_height

            # í”„ë¡¬í”„íŠ¸ ìˆœì„œì— ë”°ë¼ class_id í• ë‹¹
            label_clean = label.strip().lower()
            class_id = class_id_mapping.get(label_clean, -1) if prompt_classes else -1

            detection = {
                "class_id": class_id,  # í”„ë¡¬í”„íŠ¸ ìˆœì„œì— ë”°ë¥¸ class ID
                "class_name": label.strip(),
                "confidence": float(confidence),
                "bbox": [float(x_min), float(y_min), float(width), float(height)],
                "normalized_coords": [
                    float(x_center_norm),
                    float(y_center_norm),
                    float(width_norm),
                    float(height_norm)
                ]
            }

            detections.append(detection)

        return detections

    def predict_batch(self, images: List, **kwargs) -> List[Dict[str, Any]]:
        """
        Grounding DINO ë°°ì¹˜ ì¶”ë¡  (ì—¬ëŸ¬ ì´ë¯¸ì§€ ë™ì‹œ ì²˜ë¦¬)

        Args:
            images: PIL Image ë˜ëŠ” numpy array ë¦¬ìŠ¤íŠ¸
            **kwargs:
                - text_prompt (str): íƒì§€í•  ê°ì²´ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (ì˜ˆ: "person. car. dog.")
                - box_threshold (float): ë°•ìŠ¤ ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.3)
                - text_threshold (float): í…ìŠ¤íŠ¸ ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.25)
                - batch_size (int): ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 4)

        Returns:
            List[Dict[str, Any]]: ê° ì´ë¯¸ì§€ë³„ íƒì§€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.validate_model():
            raise HTTPException(status_code=400, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        try:
            # í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸
            text_prompt = kwargs.get('text_prompt')
            if not text_prompt:
                raise ValueError("text_promptê°€ í•„ìš”í•©ë‹ˆë‹¤ (ì˜ˆ: 'person. car. dog.')")

            box_threshold = kwargs.get('box_threshold', 0.3)
            text_threshold = kwargs.get('text_threshold', 0.25)
            batch_size = kwargs.get('batch_size', 4)

            # í”„ë¡¬í”„íŠ¸ì—ì„œ í´ë˜ìŠ¤ ìˆœì„œ ì¶”ì¶œ
            prompt_classes = [cls.strip() for cls in text_prompt.split('.') if cls.strip()]

            logger.info(f"ğŸ” Grounding DINO ë°°ì¹˜ ì¶”ë¡  ì‹œì‘")
            logger.info(f"  - ì´ ì´ë¯¸ì§€ ìˆ˜: {len(images)}ê°œ")
            logger.info(f"  - ë°°ì¹˜ í¬ê¸°: {batch_size}")
            logger.info(f"  - í”„ë¡¬í”„íŠ¸: {text_prompt}")
            logger.info(f"  - Box threshold: {box_threshold}")
            logger.info(f"  - Text threshold: {text_threshold}")

            all_results = []

            # ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for batch_idx in range(0, len(images), batch_size):
                batch_images = images[batch_idx:batch_idx + batch_size]
                batch_num = batch_idx // batch_size + 1
                total_batches = (len(images) + batch_size - 1) // batch_size

                logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(batch_images)}ê°œ ì´ë¯¸ì§€)")

                # ë°°ì¹˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                processed_images = []
                original_sizes = []

                for img in batch_images:
                    # PIL Image ë³€í™˜
                    if not isinstance(img, Image.Image):
                        if isinstance(img, np.ndarray):
                            img = Image.fromarray(img)
                        else:
                            raise ValueError("ì´ë¯¸ì§€ëŠ” PIL Image ë˜ëŠ” numpy arrayì—¬ì•¼ í•©ë‹ˆë‹¤")

                    # RGB ë³€í™˜
                    if img.mode != 'RGB':
                        img = img.convert('RGB')

                    # ì›ë³¸ í¬ê¸° ì €ì¥
                    original_sizes.append(img.size)

                    # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• (ì†ë„ ìµœì í™”)
                    max_size = 800
                    width, height = img.size

                    if max(width, height) > max_size:
                        if width > height:
                            new_width = max_size
                            new_height = int(height * (max_size / width))
                        else:
                            new_height = max_size
                            new_width = int(width * (max_size / height))
                        img = img.resize((new_width, new_height), Image.LANCZOS)

                    processed_images.append(img)

                # Processorë¡œ ë°°ì¹˜ ì…ë ¥ ì „ì²˜ë¦¬
                device = next(self.model.parameters()).device

                inputs = self.processor(
                    images=processed_images,
                    text=[text_prompt] * len(processed_images),  # ê° ì´ë¯¸ì§€ì— ë™ì¼í•œ í”„ë¡¬í”„íŠ¸
                    return_tensors="pt"
                ).to(device)

                # ë°°ì¹˜ ì¶”ë¡ 
                with torch.no_grad():
                    outputs = self.model(**inputs)

                # ë°°ì¹˜ í›„ì²˜ë¦¬
                try:
                    # ìµœì‹  ë²„ì „: box_threshold, text_threshold ì§€ì›
                    results = self.processor.post_process_grounded_object_detection(
                        outputs,
                        inputs.input_ids,
                        box_threshold=box_threshold,
                        text_threshold=text_threshold,
                        target_sizes=[img.size[::-1] for img in processed_images]
                    )
                except TypeError:
                    # êµ¬ë²„ì „: threshold íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›
                    results = self.processor.post_process_grounded_object_detection(
                        outputs,
                        inputs.input_ids,
                        target_sizes=[img.size[::-1] for img in processed_images]
                    )

                    # threshold ìˆ˜ë™ ì ìš©
                    filtered_results = []
                    for result in results:
                        filtered_boxes = []
                        filtered_scores = []
                        filtered_labels = []

                        for box, score, label in zip(result["boxes"], result["scores"], result["labels"]):
                            if float(score) >= box_threshold:
                                filtered_boxes.append(box)
                                filtered_scores.append(score)
                                filtered_labels.append(label)

                        if len(filtered_boxes) > 0:
                            filtered_results.append({
                                "boxes": torch.stack(filtered_boxes),
                                "scores": torch.stack(filtered_scores),
                                "labels": filtered_labels
                            })
                        else:
                            filtered_results.append({
                                "boxes": torch.tensor([]),
                                "scores": torch.tensor([]),
                                "labels": []
                            })
                    results = filtered_results

                # ê° ì´ë¯¸ì§€ë³„ë¡œ ê²°ê³¼ ë³€í™˜
                for idx, result in enumerate(results):
                    if len(result["boxes"]) > 0:
                        detections = self._postprocess_results(
                            boxes=result["boxes"],
                            scores=result["scores"],
                            labels=result["labels"],
                            image_size=processed_images[idx].size,
                            prompt_classes=prompt_classes,
                            original_size=original_sizes[idx]
                        )
                    else:
                        detections = []

                    all_results.append({
                        "boxes": detections,
                        "num_detections": len(detections),
                        "task_type": "bbox",
                        "model_type": "grounding_dino",
                        "text_prompt": text_prompt,
                        "prompt_classes": prompt_classes
                    })

                logger.info(f"âœ… ë°°ì¹˜ {batch_num}/{total_batches} ì™„ë£Œ")

            logger.info(f"âœ… ì „ì²´ ë°°ì¹˜ ì¶”ë¡  ì™„ë£Œ - ì´ {len(all_results)}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬")

            return all_results

        except Exception as e:
            logger.error(f"âŒ Grounding DINO ë°°ì¹˜ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")
            raise HTTPException(status_code=500, detail=f"ë°°ì¹˜ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")

    def get_model_info(self) -> Dict[str, Any]:
        """
        Grounding DINO ëª¨ë¸ ì •ë³´ ë°˜í™˜

        Returns:
            Dict[str, Any]: ëª¨ë¸ ë©”íƒ€ë°ì´í„°
        """
        return {
            "model_type": "grounding_dino",
            "task": "detection",
            "framework": "transformers",
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "is_loaded": self.is_loaded,
            "supports_text_prompt": True,
            "supports_batch_inference": True,
            "zero_shot": True,
            "model_id": self.model_id,
            "source": "Hugging Face Hub"
        }
