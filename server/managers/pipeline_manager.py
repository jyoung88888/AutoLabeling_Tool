"""
Pipeline Manager for Multi-Model Integration
ë©€í‹°ëª¨ë¸ í†µí•© íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ì
"""
from typing import Dict, Any, List, Optional
import logging
from pathlib import Path

from .model_factory import ModelFactory
from .base_model import BaseModel, TaskType

logger = logging.getLogger(__name__)


class PipelineManager:
    """
    ë©€í‹°ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ì

    ì—¬ëŸ¬ AI ëª¨ë¸ì„ í†µí•©í•˜ì—¬ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        """íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.models: Dict[str, BaseModel] = {}
        self.pipeline_config = {
            "detection": None,
            "keypoint": None,
            "ocr": None,
            "segmentation": None
        }
        logger.info("ğŸ”§ PipelineManager ì´ˆê¸°í™”")

    def add_model(
        self,
        task_name: str,
        model_name: str,
        model_path: Optional[str] = None,
        **kwargs
    ):
        """
        íŒŒì´í”„ë¼ì¸ì— ëª¨ë¸ ì¶”ê°€

        Args:
            task_name (str): ì‘ì—… ì´ë¦„ ("detection", "keypoint", "ocr" ë“±)
            model_name (str): ëª¨ë¸ íƒ€ì… ("yolo", "grounding_dino", "yolo_pose", "easyocr")
            model_path (str): ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
            **kwargs: ëª¨ë¸ë³„ ì¶”ê°€ ì„¤ì •

        Returns:
            dict: ëª¨ë¸ ë¡œë“œ ê²°ê³¼ (supports_text_prompt ë“± í¬í•¨)
        """
        try:
            logger.info(f"â• íŒŒì´í”„ë¼ì¸ì— ëª¨ë¸ ì¶”ê°€: {task_name} -> {model_name}")

            # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            model = ModelFactory.create_model(model_name)

            # ëª¨ë¸ ë¡œë“œ
            # EasyOCR, Grounding DINOëŠ” Hugging Faceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥
            auto_download_models = ["easyocr", "grounding_dino"]

            load_result = None
            if model_path or model_name not in auto_download_models:
                # ë¡œì»¬ ëª¨ë¸ íŒŒì¼ì´ í•„ìš”í•œ ê²½ìš°
                if model_path:
                    load_result = model.load_model(model_path, **kwargs)
                else:
                    raise ValueError(f"{model_name} ëª¨ë¸ì€ model_pathê°€ í•„ìš”í•©ë‹ˆë‹¤")
            else:
                # Hugging Face ë“±ì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ëª¨ë¸
                load_result = model.load_model(**kwargs)

            # íŒŒì´í”„ë¼ì¸ì— ë“±ë¡
            self.models[task_name] = model
            self.pipeline_config[task_name] = model_name

            logger.info(f"âœ… ëª¨ë¸ ì¶”ê°€ ì„±ê³µ: {task_name} -> {model_name}")

            return load_result or {"success": True}

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì¶”ê°€ ì‹¤íŒ¨ ({task_name}/{model_name}): {str(e)}")
            raise

    def remove_model(self, task_name: str):
        """
        íŒŒì´í”„ë¼ì¸ì—ì„œ ëª¨ë¸ ì œê±°

        Args:
            task_name (str): ì œê±°í•  ì‘ì—… ì´ë¦„
        """
        if task_name in self.models:
            model = self.models[task_name]
            model.unload_model()
            del self.models[task_name]
            self.pipeline_config[task_name] = None
            logger.info(f"ğŸ—‘ï¸ ëª¨ë¸ ì œê±°: {task_name}")
        else:
            logger.warning(f"âš ï¸ ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {task_name}")

    def run_pipeline(
        self,
        image,
        tasks: List[str],
        **kwargs
    ) -> Dict[str, Any]:
        """
        ë©€í‹°íƒœìŠ¤í¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€
            tasks (List[str]): ì‹¤í–‰í•  ì‘ì—… ëª©ë¡ (ì˜ˆ: ["detection", "keypoint", "ocr"])
            **kwargs: ê° ì‘ì—…ë³„ ì„¤ì •
                - detection: detection ê´€ë ¨ íŒŒë¼ë¯¸í„°
                - keypoint: keypoint ê´€ë ¨ íŒŒë¼ë¯¸í„°
                - ocr: ocr ê´€ë ¨ íŒŒë¼ë¯¸í„°

        Returns:
            Dict[str, Any]: ê° ì‘ì—…ë³„ ê²°ê³¼
        """
        logger.info(f"ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ - ì‘ì—…: {tasks}")
        results = {}

        for task in tasks:
            if task not in self.models:
                logger.warning(f"âš ï¸ {task} ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ - ìŠ¤í‚µ")
                results[task] = {
                    "error": f"{task} ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",
                    "loaded": False
                }
                continue

            try:
                model = self.models[task]
                task_kwargs = kwargs.get(task, {})

                logger.info(f"ğŸ”„ {task} ì‹¤í–‰ ì¤‘...")
                task_result = model.predict(image, **task_kwargs)
                results[task] = task_result
                logger.info(f"âœ… {task} ì™„ë£Œ")

            except Exception as e:
                logger.error(f"âŒ {task} ì‹¤íŒ¨: {str(e)}")
                results[task] = {
                    "error": str(e),
                    "success": False
                }

        logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ")
        return results

    def run_single_task(
        self,
        task_name: str,
        image,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì‘ì—… ì‹¤í–‰

        Args:
            task_name (str): ì‹¤í–‰í•  ì‘ì—… ì´ë¦„
            image: ì…ë ¥ ì´ë¯¸ì§€
            **kwargs: ì‘ì—…ë³„ ì„¤ì •

        Returns:
            Dict[str, Any]: ì‘ì—… ê²°ê³¼
        """
        if task_name not in self.models:
            raise ValueError(f"{task_name} ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        logger.info(f"ğŸ”„ ë‹¨ì¼ ì‘ì—… ì‹¤í–‰: {task_name}")
        model = self.models[task_name]
        result = model.predict(image, **kwargs)
        logger.info(f"âœ… ì‘ì—… ì™„ë£Œ: {task_name}")

        return result

    def run_batch_task(
        self,
        task_name: str,
        images: List,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ (ì—¬ëŸ¬ ì´ë¯¸ì§€ ë™ì‹œ ì²˜ë¦¬)

        Args:
            task_name (str): ì‹¤í–‰í•  ì‘ì—… ì´ë¦„
            images (List): ì…ë ¥ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            **kwargs: ì‘ì—…ë³„ ì„¤ì • (batch_size í¬í•¨)

        Returns:
            List[Dict[str, Any]]: ê° ì´ë¯¸ì§€ë³„ ì‘ì—… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if task_name not in self.models:
            raise ValueError(f"{task_name} ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        logger.info(f"ğŸ”„ ë°°ì¹˜ ì‘ì—… ì‹¤í–‰: {task_name} (ì´ë¯¸ì§€ {len(images)}ê°œ)")
        model = self.models[task_name]

        # ëª¨ë¸ì´ ë°°ì¹˜ ì¶”ë¡ ì„ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸
        if hasattr(model, 'predict_batch'):
            # ë°°ì¹˜ ì¶”ë¡  ë©”ì„œë“œ ì‚¬ìš©
            results = model.predict_batch(images, **kwargs)
        else:
            # ë°°ì¹˜ ì¶”ë¡ ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´ ìˆœì°¨ ì²˜ë¦¬
            logger.warning(f"âš ï¸ {task_name} ëª¨ë¸ì´ ë°°ì¹˜ ì¶”ë¡ ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìˆœì°¨ ì²˜ë¦¬ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            results = []
            for img in images:
                result = model.predict(img, **kwargs)
                results.append(result)

        logger.info(f"âœ… ë°°ì¹˜ ì‘ì—… ì™„ë£Œ: {task_name} (ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {len(results)}ê°œ)")

        return results

    def get_pipeline_info(self) -> Dict[str, Any]:
        """
        íŒŒì´í”„ë¼ì¸ ì •ë³´ ë°˜í™˜

        Returns:
            Dict[str, Any]: íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì •ë³´
        """
        return {
            "loaded_models": list(self.models.keys()),
            "config": self.pipeline_config,
            "model_info": {
                name: model.get_model_info()
                for name, model in self.models.items()
            },
            "num_loaded_models": len(self.models)
        }

    def is_task_loaded(self, task_name: str) -> bool:
        """
        íŠ¹ì • ì‘ì—…ì˜ ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸

        Args:
            task_name (str): í™•ì¸í•  ì‘ì—… ì´ë¦„

        Returns:
            bool: ë¡œë“œ ì—¬ë¶€
        """
        return task_name in self.models and self.models[task_name].is_loaded

    def clear_all_models(self):
        """
        ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ ë° ì •ë¦¬
        """
        logger.info("ğŸ—‘ï¸ ëª¨ë“  ëª¨ë¸ ì •ë¦¬ ì¤‘...")
        for task_name in list(self.models.keys()):
            self.remove_model(task_name)
        logger.info("âœ… ëª¨ë“  ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")

    def __str__(self):
        """ë¬¸ìì—´ í‘œí˜„"""
        loaded = [f"{k}({v})" for k, v in self.pipeline_config.items() if v]
        return f"PipelineManager(loaded={loaded})"

    def __repr__(self):
        """ê°ì²´ í‘œí˜„"""
        return self.__str__()


class PipelinePresets:
    """
    ì‚¬ì „ ì •ì˜ëœ íŒŒì´í”„ë¼ì¸ í”„ë¦¬ì…‹
    """

    @staticmethod
    def safety_inspection_pipeline() -> List[str]:
        """
        ì•ˆì „ ì ê²€ìš© íŒŒì´í”„ë¼ì¸
        (í—¬ë©§, ì•ˆì „ë³µ, ì‚¬ëŒ íƒì§€ + í¬ì¦ˆ)
        """
        return ["detection", "keypoint"]

    @staticmethod
    def document_processing_pipeline() -> List[str]:
        """
        ë¬¸ì„œ ì²˜ë¦¬ìš© íŒŒì´í”„ë¼ì¸
        (ê°ì²´ íƒì§€ + OCR)
        """
        return ["detection", "ocr"]

    @staticmethod
    def full_analysis_pipeline() -> List[str]:
        """
        ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸
        (ëª¨ë“  ì‘ì—…)
        """
        return ["detection", "keypoint", "ocr"]

    @staticmethod
    def get_preset(preset_name: str) -> List[str]:
        """
        í”„ë¦¬ì…‹ ì´ë¦„ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ê°€ì ¸ì˜¤ê¸°

        Args:
            preset_name (str): í”„ë¦¬ì…‹ ì´ë¦„

        Returns:
            List[str]: ì‘ì—… ëª©ë¡
        """
        presets = {
            "safety": PipelinePresets.safety_inspection_pipeline(),
            "document": PipelinePresets.document_processing_pipeline(),
            "full": PipelinePresets.full_analysis_pipeline()
        }
        return presets.get(preset_name, ["detection"])
